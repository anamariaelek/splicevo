# Updated training configuration for Splicevo model

# Device configuration
device: cuda
cpu_threads: 8
cudnn_benchmark: true
gpu_memory_fraction: 0.9

# Data configuration
data:
  path: /home/elek/sds/sd17d003/Anamaria/splicevo/data/splits_full_5kb/mouse_rat_human/
  use_mmap: true
  train_split: 0.8
  normalization_method: none    # SSE needs no normalization (already [0,1])
  usage_types: ['sse']          # Only SSE for usage training

# Output configuration
output:
  checkpoint_dir: /home/elek/sds/sd17d003/Anamaria/splicevo/models/transformer/full_mouse_rat_human_5kb
  use_tensorboard: true

# Model architecture
model:
  embed_dim: 128
  num_resblocks: 16
  dilation_strategy: alternating
  alternate: 4 
  bottleneck_dim: 128   # Try 128 (default), 256 (balanced), 320, or 640 (full multi-scale)
  num_heads: 8          # Transformer heads
  num_classes: 3        # Splice donor, acceptor, not a splice site
  context_len: 450      # This must match context_size in data creation (verify in metadata.json)
  dropout: 0.4

# Training configuration
training:
  # Optimizer
  learning_rate: 1.0e-3
  weight_decay: 1.0e-3
  warmup_steps: 1500      # Linear warmup from 10% to 100% of learning_rate (about 1 epoch) 

  # Loss for splice site usage (SSE)
  usage_loss_type: 'weighted_mse'  # 'mse' or 'bce' or 'weighted_mse' or 'hybrid'

  # Weighted MSE parameters
  weighted_mse_extreme_low: 0.1        # Values < this are extreme near zero
  weighted_mse_extreme_high: 0.9       # Values > this are extreme near one
  weighted_mse_extreme_weight: 5.0     # Weight for errors in extreme values (weight for other values=1)

  # Hybrid loss parameters
  hybrid_extreme_low: 0.1       # Values < this classified as "zero"
  hybrid_extreme_high: 0.9      # Values > this classified as "one"
  hybrid_class_weight: 0.8      # Weight for classification component
  hybrid_reg_weight: 0.2        # Weight for regression component

  # How to combine splice site and usage loss
  splice_weight: 0.5
  usage_weight: 0.5

  # Class balancing (not a splice site, aceptor, donor)
  use_class_weights: true
  
  # Training loop
  n_epochs: 50
  early_stopping_patience: 5
  
  # Memory optimization
  use_amp: true
  gradient_accumulation_steps: 8
  
  # DataLoader
  # Full dataset: 99 conditions (vs 8 in small) = 12.4× larger usage tensors!
  # batch=16 with 99 conditions: ~40-50 GB → OOM on A100 80GB
  # batch=8 with 99 conditions: ~20-25 GB (safe)
  # With gradient_accumulation_steps=8: effective batch = 8*8 = 64
  dataloader:
    batch_size: 8
    num_workers: 6
    prefetch_factor: 2
