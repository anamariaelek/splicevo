Device: cpu

Loading 256 samples from /home/elek/projects/splicing/results/data_processing_subset/processed_data_train.npz...
  Step 1: Per-sample CPM normalization...
  Step 2: Log1p + standardization (using all data)...
    → log_mean=12.784, log_std=2.677
    → Valid values: 552 / 1,024,000 (0.1%)
  Step 1: Per-sample CPM normalization...
  Step 2: Log1p + standardization (using all data)...
    → log_mean=11.414, log_std=4.713
    → Valid values: 552 / 1,024,000 (0.1%)

Original usage array stats:
  usage_alpha: min=0.000, max=1125.000, mean=46.321, std=81.515
    - 25th percentile: 4.000
    - 50th percentile: 16.000
    - 75th percentile: 54.250
  usage_beta: min=0.000, max=165.000, mean=12.350, std=23.911
    - 25th percentile: 1.750
    - 50th percentile: 4.000
    - 75th percentile: 11.000
  usage_sse: min=0.000, max=1.000, mean=0.687, std=0.338
    - 25th percentile: 0.476
    - 50th percentile: 0.837
    - 75th percentile: 0.966

Normalized usage array stats:
  usage_alpha: min=-4.775, max=0.385, mean=-0.000, std=1.000
    - 25th percentile: 0.051
    - 50th percentile: 0.385
    - 75th percentile: 0.385
  usage_beta: min=-2.422, max=0.510, mean=-0.000, std=1.000
    - 25th percentile: 0.215
    - 50th percentile: 0.440
    - 75th percentile: 0.510
  usage_sse: min=0.000, max=1.000, mean=0.687, std=0.338
    - 25th percentile: 0.476
    - 50th percentile: 0.837
    - 75th percentile: 0.966
Data shapes:
  sequences: (256, 10000, 4)
  labels: (256, 1000)
  usage_alpha: (256, 1000, 4)
  usage_beta: (256, 1000, 4)
  usage_sse: (256, 1000, 4)

DataLoader created:
  batch_size: 32
  n_batches: 8

Model created with 1,835,919 parameters
Batch 0 keys:
  - sequences
  - splice_labels
  - usage_targets

============================================================
BATCH 0
============================================================
Sequences shape: torch.Size([32, 10000, 4])
  - dtype: torch.float32
  - range: [0.000, 1.000]
  - mean: 0.250

Labels shape: torch.Size([32, 1000])
  - dtype: torch.int64
  - unique values: [0, 1, 2]
  - class distribution:
    Class 0:  31932 ( 99.8%)
    Class 1:     35 (  0.1%)
    Class 2:     33 (  0.1%)

Usage arrays:
  alpha: torch.Size([32, 4, 3])
    - all values are NaN
  beta: torch.Size([32, 4, 3])
    - all values are NaN
  sse: torch.Size([32, 4, 3])
    - all values are NaN

============================================================
TRAINING STEP
============================================================

Losses:
  splice_loss: 1.440194
  usage_loss_alpha: nan
  usage_loss_beta: nan
  usage_loss_sse: nan
  usage_loss (avg): nan
  total_loss: nan

Backward pass...
  gradient norms: min=nan, max=nan, mean=nan
  optimizer step complete
Batch 1 keys:
  - sequences
  - splice_labels
  - usage_targets

============================================================
BATCH 1
============================================================
Sequences shape: torch.Size([32, 10000, 4])
  - dtype: torch.float32
  - range: [0.000, 1.000]
  - mean: 0.250

Labels shape: torch.Size([32, 1000])
  - dtype: torch.int64
  - unique values: [0, 1, 2]
  - class distribution:
    Class 0:  31943 ( 99.8%)
    Class 1:     30 (  0.1%)
    Class 2:     27 (  0.1%)

Usage arrays:
  alpha: torch.Size([32, 4, 3])
    - all values are NaN
  beta: torch.Size([32, 4, 3])
    - all values are NaN
  sse: torch.Size([32, 4, 3])
    - all values are NaN

============================================================
TRAINING STEP
============================================================

Losses:
  splice_loss: nan
  usage_loss_alpha: nan
  usage_loss_beta: nan
  usage_loss_sse: nan
  usage_loss (avg): nan
  total_loss: nan

Backward pass...
  gradient norms: min=nan, max=nan, mean=nan
  optimizer step complete
Batch 2 keys:
  - sequences
  - splice_labels
  - usage_targets

============================================================
BATCH 2
============================================================
Sequences shape: torch.Size([32, 10000, 4])
  - dtype: torch.float32
  - range: [0.000, 1.000]
  - mean: 0.250

Labels shape: torch.Size([32, 1000])
  - dtype: torch.int64
  - unique values: [0, 1, 2]
  - class distribution:
    Class 0:  31931 ( 99.8%)
    Class 1:     34 (  0.1%)
    Class 2:     35 (  0.1%)

Usage arrays:
  alpha: torch.Size([32, 4, 3])
    - all values are NaN
  beta: torch.Size([32, 4, 3])
    - all values are NaN
  sse: torch.Size([32, 4, 3])
    - all values are NaN

============================================================
TRAINING STEP
============================================================

Losses:
  splice_loss: nan
  usage_loss_alpha: nan
  usage_loss_beta: nan
  usage_loss_sse: nan
  usage_loss (avg): nan
  total_loss: nan

Backward pass...
  gradient norms: min=nan, max=nan, mean=nan
  optimizer step complete
Batch 3 keys:
  - sequences
  - splice_labels
  - usage_targets

============================================================
BATCH 3
============================================================
Sequences shape: torch.Size([32, 10000, 4])
  - dtype: torch.float32
  - range: [0.000, 1.000]
  - mean: 0.250

Labels shape: torch.Size([32, 1000])
  - dtype: torch.int64
  - unique values: [0, 1, 2]
  - class distribution:
    Class 0:  31911 ( 99.7%)
    Class 1:     44 (  0.1%)
    Class 2:     45 (  0.1%)

Usage arrays:
  alpha: torch.Size([32, 4, 3])
    - all values are NaN
  beta: torch.Size([32, 4, 3])
    - all values are NaN
  sse: torch.Size([32, 4, 3])
    - all values are NaN

============================================================
TRAINING STEP
============================================================

Losses:
  splice_loss: nan
  usage_loss_alpha: nan
  usage_loss_beta: nan
  usage_loss_sse: nan
  usage_loss (avg): nan
  total_loss: nan

Backward pass...
  gradient norms: min=nan, max=nan, mean=nan
  optimizer step complete

============================================================
DEBUG COMPLETE
============================================================
