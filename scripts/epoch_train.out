Device: cpu

Loading 100 samples from /home/elek/projects/splicing/results/data_processing/processed_data_test.npz...
  Step 1: Per-sample CPM normalization...
  Step 2: Log1p + standardization (using all data)...
    → log_mean=10.881, log_std=4.619
    → Valid values: 9,624 / 6,200,000 (0.2%)
  Step 1: Per-sample CPM normalization...
  Step 2: Log1p + standardization (using all data)...
    → log_mean=10.312, log_std=5.026
    → Valid values: 9,624 / 6,200,000 (0.2%)

Original usage array stats:
  usage_alpha: min=0.000, max=2481.000, mean=48.091, std=107.639
    - 25th percentile: 1.000
    - 50th percentile: 9.000
    - 75th percentile: 42.000
  usage_beta: min=0.000, max=989.000, mean=10.995, std=40.770
    - 25th percentile: 1.000
    - 50th percentile: 3.000
    - 75th percentile: 8.000
  usage_sse: min=0.000, max=1.000, mean=0.636, std=0.376
    - 25th percentile: 0.312
    - 50th percentile: 0.806
    - 75th percentile: 0.959

Normalized usage array stats:
  usage_alpha: min=-2.356, max=0.635, mean=-0.000, std=1.000
    - 25th percentile: 0.209
    - 50th percentile: 0.416
    - 75th percentile: 0.515
  usage_beta: min=-2.052, max=0.697, mean=0.000, std=1.000
    - 25th percentile: 0.169
    - 50th percentile: 0.442
    - 75th percentile: 0.590
  usage_sse: min=0.000, max=1.000, mean=0.636, std=0.376
    - 25th percentile: 0.312
    - 50th percentile: 0.806
    - 75th percentile: 0.959
Data shapes:
  sequences: (100, 10000, 4)
  labels: (100, 1000)
  usage_alpha: (100, 1000, 62)
  usage_beta: (100, 1000, 62)
  usage_sse: (100, 1000, 62)

DataLoader created:
  batch_size: 32
  n_batches: 4

Model created with 1,858,365 parameters
Batch 0 keys:
  - sequences
  - splice_labels
  - usage_targets

============================================================
BATCH 0
============================================================
Sequences shape: torch.Size([32, 10000, 4])

Labels shape: torch.Size([32, 1000])
  - dtype: torch.int64
  - unique values: [0, 1, 2]
  - class distribution:
    Class 0:  31927 ( 99.8%)
    Class 1:     40 (  0.1%)
    Class 2:     33 (  0.1%)

Usage arrays:
  usage type: torch.FloatTensor
  usage shape: torch.Size([32, 1000, 62, 3])
  alpha: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 3316
    - min: -2.356
    - max: 0.635
    - mean: 0.070
    - std: 0.907
    - 25th percentile: 0.236
    - 50th percentile: 0.402
    - 75th percentile: 0.513
  beta: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 3316
    - min: -2.052
    - max: 0.697
    - mean: 0.058
    - std: 0.931
    - 25th percentile: 0.179
    - 50th percentile: 0.453
    - 75th percentile: 0.564
  sse: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 3316
    - min: 0.000
    - max: 1.000
    - mean: 0.663
    - std: 0.367
    - 25th percentile: 0.400
    - 50th percentile: 0.840
    - 75th percentile: 0.962

============================================================
TRAINING STEP
============================================================

Losses:
  splice_loss: 0.983172
  usage_loss_alpha: 1.254747
  usage_loss_beta: 1.338979
  usage_loss_sse: 1.127238
  usage_loss (avg): 1.240322
  total_loss: 1.603333

Backward pass...
  gradient norms: min=0.000000, max=8.580789, mean=0.639174
  optimizer step complete
Batch 1 keys:
  - sequences
  - splice_labels
  - usage_targets

============================================================
BATCH 1
============================================================
Sequences shape: torch.Size([32, 10000, 4])

Labels shape: torch.Size([32, 1000])
  - dtype: torch.int64
  - unique values: [0, 1, 2]
  - class distribution:
    Class 0:  31924 ( 99.8%)
    Class 1:     38 (  0.1%)
    Class 2:     38 (  0.1%)

Usage arrays:
  usage type: torch.FloatTensor
  usage shape: torch.Size([32, 1000, 62, 3])
  alpha: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 2809
    - min: -2.356
    - max: 0.635
    - mean: -0.157
    - std: 1.151
    - 25th percentile: 0.105
    - 50th percentile: 0.398
    - 75th percentile: 0.534
  beta: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 2809
    - min: -2.052
    - max: 0.697
    - mean: -0.064
    - std: 1.066
    - 25th percentile: 0.220
    - 50th percentile: 0.421
    - 75th percentile: 0.595
  sse: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 2809
    - min: 0.000
    - max: 1.000
    - mean: 0.554
    - std: 0.378
    - 25th percentile: 0.154
    - 50th percentile: 0.645
    - 75th percentile: 0.923

============================================================
TRAINING STEP
============================================================

Losses:
  splice_loss: 0.924857
  usage_loss_alpha: 1.454686
  usage_loss_beta: 1.377173
  usage_loss_sse: 0.920951
  usage_loss (avg): 1.250937
  total_loss: 1.550325

Backward pass...
  gradient norms: min=0.000000, max=8.247949, mean=0.613136
  optimizer step complete
Batch 2 keys:
  - sequences
  - splice_labels
  - usage_targets

============================================================
BATCH 2
============================================================
Sequences shape: torch.Size([32, 10000, 4])

Labels shape: torch.Size([32, 1000])
  - dtype: torch.int64
  - unique values: [0, 1, 2]
  - class distribution:
    Class 0:  31931 ( 99.8%)
    Class 1:     38 (  0.1%)
    Class 2:     31 (  0.1%)

Usage arrays:
  usage type: torch.FloatTensor
  usage shape: torch.Size([32, 1000, 62, 3])
  alpha: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 2783
    - min: -2.356
    - max: 0.635
    - mean: 0.042
    - std: 0.985
    - 25th percentile: 0.249
    - 50th percentile: 0.469
    - 75th percentile: 0.541
  beta: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 2783
    - min: -2.052
    - max: 0.697
    - mean: 0.020
    - std: 1.004
    - 25th percentile: 0.101
    - 50th percentile: 0.502
    - 75th percentile: 0.628
  sse: torch.Size([32, 1000, 62])
    - num valid data points (non-NaN): 2783
    - min: 0.000
    - max: 1.000
    - mean: 0.647
    - std: 0.390
    - 25th percentile: 0.220
    - 50th percentile: 0.857
    - 75th percentile: 0.965

============================================================
TRAINING STEP
============================================================

Losses:
  splice_loss: 0.881180
  usage_loss_alpha: 1.283349
  usage_loss_beta: 1.369433
  usage_loss_sse: 1.082360
  usage_loss (avg): 1.245047
  total_loss: 1.503704

Backward pass...
  gradient norms: min=0.000000, max=8.052278, mean=0.601761
  optimizer step complete

============================================================
DEBUG COMPLETE
============================================================
